@article{hauserFacultyLanguageWhat2002,
  title = {The {{Faculty}} of {{Language}}: {{What Is It}}, {{Who Has It}}, and {{How Did It Evolve}}?},
  shorttitle = {The {{Faculty}} of {{Language}}},
  author = {Hauser, Marc D. and Chomsky, Noam and Fitch, W. Tecumseh},
  year = {2002},
  month = nov,
  journal = {Science},
  volume = {298},
  number = {5598},
  pages = {1569--1579},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.298.5598.1569},
  urldate = {2022-07-06},
  abstract = {We argue that an understanding of the faculty of language requires substantial interdisciplinary cooperation. We suggest how current developments in linguistics can be profitably wedded to work in evolutionary biology, anthropology, psychology, and neuroscience. We submit that a distinction should be made between the faculty of language in the broad sense (FLB) and in the narrow sense (FLN). FLB includes a sensory-motor system, a conceptual-intentional system, and the computational mechanisms for recursion, providing the capacity to generate an infinite range of expressions from a finite set of elements. We hypothesize that FLN only includes recursion and is the only uniquely human component of the faculty of language. We further argue that FLN may have evolved for reasons other than language, hence comparative studies might look for evidence of such computations outside of the domain of communication (for example, number, navigation, and social relations).},
  langid = {english},
  file = {/Users/davidnicholson/Zotero/storage/HY959NW7/Hauser et al. - 2002 - The Faculty of Language What Is It, Who Has It, a.pdf}
}
@article{doupeBIRDSONGHUMANSPEECH1999,
  title = {{{BIRDSONG AND HUMAN SPEECH}}: {{Common Themes}} and {{Mechanisms}}},
  shorttitle = {{{BIRDSONG AND HUMAN SPEECH}}},
  author = {Doupe, Allison J. and Kuhl, Patricia K.},
  year = {1999},
  month = mar,
  journal = {Annual Review of Neuroscience},
  volume = {22},
  number = {1},
  pages = {567--631},
  issn = {0147-006X, 1545-4126},
  doi = {10.1146/annurev.neuro.22.1.567},
  urldate = {2023-05-10},
  abstract = {Human speech and birdsong have numerous parallels. Both humans and songbirds learn their complex vocalizations early in life, exhibiting a strong dependence on hearing the adults they will imitate, as well as themselves as they practice, and a waning of this dependence as they mature. Innate predispositions for perceiving and learning the correct sounds exist in both groups, although more evidence of innate descriptions of species-specific signals exists in songbirds, where numerous species of vocal learners have been compared. Humans also share with songbirds an early phase of learning that is primarily perceptual, which then serves to guide later vocal production. Both humans and songbirds have evolved a complex hierarchy of specialized forebrain areas in which motor and auditory centers interact closely, and which control the lower vocal motor areas also found in nonlearners. In both these vocal learners, however, how auditory feedback of self is processed in these brain areas is surprisingly unclear. Finally, humans and songbirds have similar critical periods for vocal learning, with a much greater ability to learn early in life. In both groups, the capacity for late vocal learning may be decreased by the act of learning itself, as well as by biological factors such as the hormones of puberty. Although some features of birdsong and speech are clearly not analogous, such as the capacity of language for meaning, abstraction, and flexible associations, there are striking similarities in how sensory experience is internalized and used to shape vocal outputs, and how learning is enhanced during a critical period of development. Similar neural mechanisms may therefore be involved.},
  langid = {english},
  file = {/Users/davidnicholson/Zotero/storage/L528VXUH/Doupe and Kuhl - 1999 - BIRDSONG AND HUMAN SPEECH Common Themes and Mecha.pdf}
}
@article{wirthlinModularApproachVocal2019,
  title = {A {{Modular Approach}} to {{Vocal Learning}}: {{Disentangling}} the {{Diversity}} of a {{Complex Behavioral Trait}}},
  shorttitle = {A {{Modular Approach}} to {{Vocal Learning}}},
  author = {Wirthlin, Morgan and Chang, Edward F. and Kn{\"o}rnschild, Mirjam and Krubitzer, Leah A. and Mello, Claudio V. and Miller, Cory T. and Pfenning, Andreas R. and Vernes, Sonja C. and Tchernichovski, Ofer and Yartsev, Michael M.},
  year = {2019},
  month = oct,
  journal = {Neuron},
  volume = {104},
  number = {1},
  pages = {87--99},
  issn = {08966273},
  doi = {10.1016/j.neuron.2019.09.036},
  urldate = {2021-05-22},
  langid = {english},
  file = {/Users/davidnicholson/Zotero/storage/L7JW7EGY/Wirthlin et al. - 2019 - A Modular Approach to Vocal Learning Disentanglin.pdf}
}
@book{hopp2012animal,
  title = {Animal Acoustic Communication: Sound Analysis and Research Methods},
  author = {Hopp, Steven L and Owren, Michael J and Evans, Christopher S},
  year = {2012},
  publisher = {{Springer Science \& Business Media}}
}
@article{sainburgComputationalNeuroethologyVocal2021,
  title = {Toward a {{Computational Neuroethology}} of {{Vocal Communication}}: {{From Bioacoustics}} to {{Neurophysiology}}, {{Emerging Tools}} and {{Future Directions}}},
  shorttitle = {Toward a {{Computational Neuroethology}} of {{Vocal Communication}}},
  author = {Sainburg, Tim and Gentner, Timothy Q.},
  year = {2021},
  month = dec,
  journal = {Frontiers in Behavioral Neuroscience},
  volume = {15},
  pages = {811737},
  issn = {1662-5153},
  doi = {10.3389/fnbeh.2021.811737},
  urldate = {2022-01-22},
  abstract = {Recently developed methods in computational neuroethology have enabled increasingly detailed and comprehensive quantification of animal movements and behavioral kinematics. Vocal communication behavior is well poised for application of similar large-scale quantification methods in the service of physiological and ethological studies. This review describes emerging techniques that can be applied to acoustic and vocal communication signals with the goal of enabling study beyond a small number of model species. We review a range of modern computational methods for bioacoustics, signal processing, and brain-behavior mapping. Along with a discussion of recent advances and techniques, we include challenges and broader goals in establishing a framework for the computational neuroethology of vocal communication.},
  langid = {english},
  file = {/Users/davidnicholson/Zotero/storage/AZFQEZ85/Sainburg and Gentner - 2021 - Toward a Computational Neuroethology of Vocal Comm.pdf}
}
@article{stowellComputationalBioacousticsDeep2022,
  title = {Computational Bioacoustics with Deep Learning: A Review and Roadmap},
  author = {Stowell, Dan},
  year = {2022},
  pages = {46},
  abstract = {Animal vocalisations and natural soundscapes are fascinating objects of study, and contain valuable evidence about animal behaviours, populations and ecosystems. They are studied in bioacoustics and ecoacoustics, with signal processing and analysis an important component. Computational bioacoustics has accelerated in recent decades due to the growth of affordable digital sound recording devices, and to huge progress in informatics such as big data, signal processing and machine learning. Methods are inherited from the wider field of deep learning, including speech and image processing. However, the tasks, demands and data characteristics are often different from those addressed in speech or music analysis. There remain unsolved problems, and tasks for which evidence is surely present in many acoustic signals, but not yet realised. In this paper I perform a review of the state of the art in deep learning for computational bioacoustics, aiming to clarify key concepts and identify and analyse knowledge gaps. Based on this, I offer a subjective but principled roadmap for computational bioacoustics with deep learning: topics that the community should aim to address, in order to make the most of future developments in AI and informatics, and to use audio data in answering zoological and ecological questions.},
  langid = {english},
  file = {/Users/davidnicholson/Zotero/storage/PAAGY2CU/Stowell - 2022 - Computational bioacoustics with deep learning a r.pdf}
}
@article{cohen2022recent,
  title={Recent Advances at the Interface of Neuroscience and Artificial Neural Networks},
  author={Cohen, Yarden and Engel, Tatiana A and Langdon, Christopher and Lindsay, Grace W and Ott, Torben and Peters, Megan AK and Shine, James M and Breton-Provencher, Vincent and Ramaswamy, Srikanth},
  journal={Journal of Neuroscience},
  volume={42},
  number={45},
  pages={8514--8523},
  year={2022},
  publisher={Soc Neuroscience}
}
@article{cohenAutomatedAnnotationBirdsong2022,
  title = {Automated Annotation of Birdsong with a Neural Network That Segments Spectrograms},
  author = {Cohen, Yarden and Nicholson, David Aaron and Sanchioni, Alexa and Mallaber, Emily K and Skidanova, Viktoriya and Gardner, Timothy J},
  year = {2022},
  journal = {Elife},
  volume = {11},
  pages = {e63853},
  publisher = {{eLife Sciences Publications Limited}}
}
@article{goffinetLowdimensionalLearnedFeature2021,
  title = {Low-Dimensional Learned Feature Spaces Quantify Individual and Group Differences in Vocal Repertoires},
  author = {Goffinet, Jack and Brudner, Samuel and Mooney, Richard and Pearson, John},
  year = {2021},
  month = may,
  journal = {eLife},
  volume = {10},
  pages = {e67855},
  issn = {2050-084X},
  doi = {10.7554/eLife.67855},
  urldate = {2022-08-09},
  abstract = {Increases in the scale and complexity of behavioral data pose an increasing challenge for data analysis. A common strategy involves replacing entire behaviors with small numbers of handpicked, domain-\-specific features, but this approach suffers from several crucial limitations. For example, handpicked features may miss important dimensions of variability, and correlations among them complicate statistical testing. Here, by contrast, we apply the variational autoencoder (VAE), an unsupervised learning method, to learn features directly from data and quantify the vocal behavior of two model species: the laboratory mouse and the zebra finch. The VAE converges on a parsimonious representation that outperforms handpicked features on a variety of common analysis tasks, enables the measurement of moment-\-by-m\- oment vocal variability on the timescale of tens of milliseconds in the zebra finch, provides strong evidence that mouse ultrasonic vocalizations do not cluster as is commonly believed, and captures the similarity of tutor and pupil birdsong with qualitatively higher fidelity than previous approaches. In all, we demonstrate the utility of modern unsupervised learning approaches to the quantification of complex and high-\-dimensional vocal behavior.},
  langid = {english},
  file = {/Users/davidnicholson/Zotero/storage/8KFUVGLW/Goffinet et al. - 2021 - Low-dimensional learned feature spaces quantify in.pdf}
}
@article{steinfathFastAccurateAnnotation2021,
  title = {Fast and Accurate Annotation of Acoustic Signals with Deep Neural Networks},
  author = {Steinfath, Elsa and {Palacios-Mu{\~n}oz}, Adrian and Rottsch{\"a}fer, Julian R and Yuezak, Deniz and Clemens, Jan},
  editor = {Calabrese, Ronald L and Egnor, SE Roian and Troyer, Todd},
  year = {2021},
  month = nov,
  journal = {eLife},
  volume = {10},
  pages = {e68837},
  publisher = {{eLife Sciences Publications, Ltd}},
  issn = {2050-084X},
  doi = {10.7554/eLife.68837},
  urldate = {2023-03-09},
  abstract = {Acoustic signals serve communication within and across species throughout the animal kingdom. Studying the genetics, evolution, and neurobiology of acoustic communication requires annotating acoustic signals: segmenting and identifying individual acoustic elements like syllables or sound pulses. To be useful, annotations need to be accurate, robust to noise, and fast. We here introduce DeepAudioSegmenter (DAS), a method that annotates acoustic signals across species based on a deep-learning derived hierarchical presentation of sound. We demonstrate the accuracy, robustness, and speed of DAS using acoustic signals with diverse characteristics from insects, birds, and mammals. DAS comes with a graphical user interface for annotating song, training the network, and for generating and proofreading annotations. The method can be trained to annotate signals from new species with little manual annotation and can be combined with unsupervised methods to discover novel signal types. DAS annotates song with high throughput and low latency for experimental interventions in realtime. Overall, DAS is a universal, versatile, and accessible tool for annotating acoustic communication signals.},
  keywords = {acoustic communication,annotation,bird,deep learning,fly,song},
  file = {/Users/davidnicholson/Zotero/storage/5BMYZ5SB/Steinfath et al. - 2021 - Fast and accurate annotation of acoustic signals w.pdf}
}
@article{sainburgFindingVisualizingQuantifying2020,
  title = {Finding, Visualizing, and Quantifying Latent Structure across Diverse Animal Vocal Repertoires},
  author = {Sainburg, Tim and Thielk, Marvin and Gentner, Timothy Q.},
  year = {2020},
  month = oct,
  journal = {PLOS Computational Biology},
  volume = {16},
  number = {10},
  pages = {e1008228},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1008228},
  urldate = {2023-03-09},
  abstract = {Animals produce vocalizations that range in complexity from a single repeated call to hundreds of unique vocal elements patterned in sequences unfolding over hours. Characterizing complex vocalizations can require considerable effort and a deep intuition about each species' vocal behavior. Even with a great deal of experience, human characterizations of animal communication can be affected by human perceptual biases. We present a set of computational methods for projecting animal vocalizations into low dimensional latent representational spaces that are directly learned from the spectrograms of vocal signals. We apply these methods to diverse datasets from over 20 species, including humans, bats, songbirds, mice, cetaceans, and nonhuman primates. Latent projections uncover complex features of data in visually intuitive and quantifiable ways, enabling high-powered comparative analyses of vocal acoustics. We introduce methods for analyzing vocalizations as both discrete sequences and as continuous latent variables. Each method can be used to disentangle complex spectro-temporal structure and observe long-timescale organization in communication.},
  langid = {english},
  keywords = {Animal communication,Bioacoustics,Birds,Finches,Hidden Markov models,Speech,Syllables,Vocalization},
  file = {/Users/davidnicholson/Zotero/storage/B5AANVYI/Sainburg et al. - 2020 - Finding, visualizing, and quantifying latent struc.pdf}
}
@article{coffeyDeepSqueakDeepLearningbased2019,
  title = {{{DeepSqueak}}: A Deep Learning-Based System for Detection and Analysis of Ultrasonic Vocalizations},
  shorttitle = {{{DeepSqueak}}},
  author = {Coffey, Kevin R. and Marx, Ruby E. and Neumaier, John F.},
  year = {2019},
  month = apr,
  journal = {Neuropsychopharmacology},
  volume = {44},
  number = {5},
  pages = {859--868},
  issn = {0893-133X, 1740-634X},
  doi = {10.1038/s41386-018-0303-6},
  urldate = {2023-03-09},
  langid = {english},
  file = {/Users/davidnicholson/Zotero/storage/E9TXEBJK/Coffey et al. - 2019 - DeepSqueak a deep learning-based system for detec.pdf}
}
@misc{nicholsonVak2022,
  title = {Vak},
  author = {Nicholson, David and Cohen, Yarden},
  year = {2022},
  month = mar,
  doi = {10.5281/zenodo.6808839},
  howpublished = {Zenodo}
}
@article{mcgregorSharedMechanismsAuditory2022,
  title = {Shared Mechanisms of Auditory and Non-Auditory Vocal Learning in the Songbird Brain},
  author = {McGregor, James N and Grassler, Abigail L and Jaffe, Paul I and Jacob, Amanda Louise and Brainard, Michael S and Sober, Samuel J},
  editor = {Goldberg, Jesse H and {Shinn-Cunningham}, Barbara G and Giret, Nicolas},
  year = {2022},
  month = sep,
  journal = {eLife},
  volume = {11},
  pages = {e75691},
  publisher = {{eLife Sciences Publications, Ltd}},
  issn = {2050-084X},
  doi = {10.7554/eLife.75691},
  urldate = {2023-03-09},
  abstract = {Songbirds and humans share the ability to adaptively modify their vocalizations based on sensory feedback. Prior studies have focused primarily on the role that auditory feedback plays in shaping vocal output throughout life. In contrast, it is unclear how non-auditory information drives vocal plasticity. Here, we first used a reinforcement learning paradigm to establish that somatosensory feedback (cutaneous electrical stimulation) can drive vocal learning in adult songbirds. We then assessed the role of a songbird basal ganglia thalamocortical pathway critical to auditory vocal learning in this novel form of vocal plasticity. We found that both this circuit and its dopaminergic inputs are necessary for non-auditory vocal learning, demonstrating that this pathway is critical for guiding adaptive vocal changes based on both auditory and somatosensory signals. The ability of this circuit to use both auditory and somatosensory information to guide vocal learning may reflect a general principle for the neural systems that support vocal plasticity across species.},
  keywords = {Bengalese finch,lonchura striata var. domestica,songbird},
  file = {/Users/davidnicholson/Zotero/storage/VMYTLYGD/McGregor et al. - 2022 - Shared mechanisms of auditory and non-auditory voc.pdf}
}
@article{provostImpactsFinetuningPhylogenetic2022,
  title = {The Impacts of Fine-Tuning, Phylogenetic Distance, and Sample Size on Big-Data Bioacoustics},
  author = {Provost, Kaiya L. and Yang, Jiaying and Carstens, Bryan C.},
  year = {2022},
  month = dec,
  journal = {PLOS ONE},
  volume = {17},
  number = {12},
  pages = {e0278522},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0278522},
  urldate = {2023-03-09},
  abstract = {Vocalizations in animals, particularly birds, are critically important behaviors that influence their reproductive fitness. While recordings of bioacoustic data have been captured and stored in collections for decades, the automated extraction of data from these recordings has only recently been facilitated by artificial intelligence methods. These have yet to be evaluated with respect to accuracy of different automation strategies and features. Here, we use a recently published machine learning framework to extract syllables from ten bird species ranging in their phylogenetic relatedness from 1 to 85 million years, to compare how phylogenetic relatedness influences accuracy. We also evaluate the utility of applying trained models to novel species. Our results indicate that model performance is best on conspecifics, with accuracy progressively decreasing as phylogenetic distance increases between taxa. However, we also find that the application of models trained on multiple distantly related species can improve the overall accuracy to levels near that of training and analyzing a model on the same species. When planning big-data bioacoustics studies, care must be taken in sample design to maximize sample size and minimize human labor without sacrificing accuracy.},
  langid = {english},
  keywords = {Animal phylogenetics,Bioacoustics,Bird song,Birds,Machine learning,Machine learning algorithms,Syllables,Vocalization},
  file = {/Users/davidnicholson/Zotero/storage/JM8VWYDR/Provost et al. - 2022 - The impacts of fine-tuning, phylogenetic distance,.pdf}
}
@article{kershenbaumAcousticSequencesNonhuman2016,
  title = {Acoustic Sequences in Non-Human Animals: A Tutorial Review and Prospectus: {{Acoustic}} Sequences in Animals},
  shorttitle = {Acoustic Sequences in Non-Human Animals},
  author = {Kershenbaum, Arik and Blumstein, Daniel T. and Roch, Marie A. and Ak{\c c}ay, {\c C}a{\u g}lar and Backus, Gregory and Bee, Mark A. and Bohn, Kirsten and Cao, Yan and Carter, Gerald and C{\"a}sar, Cristiane and Coen, Michael and DeRuiter, Stacy L. and Doyle, Laurance and Edelman, Shimon and {Ferrer-i-Cancho}, Ramon and Freeberg, Todd M. and Garland, Ellen C. and Gustison, Morgan and Harley, Heidi E. and Huetz, Chlo{\'e} and Hughes, Melissa and Hyland Bruno, Julia and Ilany, Amiyaal and Jin, Dezhe Z. and Johnson, Michael and Ju, Chenghui and Karnowski, Jeremy and Lohr, Bernard and Manser, Marta B. and McCowan, Brenda and Mercado, Eduardo and Narins, Peter M. and Piel, Alex and Rice, Megan and Salmi, Roberta and Sasahara, Kazutoshi and Sayigh, Laela and Shiu, Yu and Taylor, Charles and Vallejo, Edgar E. and Waller, Sara and {Zamora-Gutierrez}, Veronica},
  year = {2016},
  month = feb,
  journal = {Biological Reviews},
  volume = {91},
  number = {1},
  pages = {13--52},
  issn = {14647931},
  doi = {10.1111/brv.12160},
  urldate = {2021-11-22},
  abstract = {Animal acoustic communication often takes the form of complex sequences, made up of multiple distinct acoustic units. Apart from the well-known example of birdsong, other animals such as insects, amphibians, and mammals (including bats, rodents, primates, and cetaceans) also generate complex acoustic sequences. Occasionally, such as with birdsong, the adaptive role of these sequences seems clear (e.g. mate attraction and territorial defence). More often however, researchers have only begun to characterise \textendash{} let alone understand \textendash{} the significance and meaning of acoustic sequences. Hypotheses abound, but there is little agreement as to how sequences should be defined and analysed. Our review aims to outline suitable methods for testing these hypotheses, and to describe the major limitations to our current and near-future knowledge on questions of acoustic sequences. This review and prospectus is the result of a collaborative effort between 43 scientists from the fields of animal behaviour, ecology and evolution, signal processing, machine learning, quantitative linguistics, and information theory, who gathered for a 2013 workshop entitled, `Analysing vocal sequences in animals'. Our goal is to present not just a review of the state of the art, but to propose a methodological framework that summarises what we suggest are the best practices for research in this field, across taxa and across disciplines. We also provide a tutorial-style introduction to some of the most promising algorithmic approaches for analysing sequences. We divide our review into three sections: identifying the distinct units of an acoustic sequence, describing the different ways that information can be contained within a sequence, and analysing the structure of that sequence. Each of these sections is further subdivided to address the key questions and approaches in that area. We propose a uniform, systematic, and comprehensive approach to studying sequences, with the goal of clarifying research terms used in different fields, and facilitating collaboration and comparative studies. Allowing greater interdisciplinary collaboration will facilitate the investigation of many important questions in the evolution of communication and sociality.},
  langid = {english},
  file = {/Users/davidnicholson/Zotero/storage/ABH3RYWI/Kershenbaum et al. - 2016 - Acoustic sequences in non-human animals a tutoria.pdf}
}
@phdthesis{fukuzawaComputationalMethodsGeneralised2022,
  title = {Computational Methods for a Generalised Acoustics Analysis Workflow: A Thesis Presented in Partial Fulfilment of the Requirements for the Degree of {{Master}} of {{Science}} in {{Computer Science}} at {{Massey University}}, {{Auckland}}, {{New Zealand}}},
  shorttitle = {Computational Methods for a Generalised Acoustics Analysis Workflow},
  author = {Fukuzawa, Yukio},
  year = {2022},
  school = {Massey University},
  file = {/Users/davidnicholson/Zotero/storage/8PCXKJU5/FukuzawaMPhilThesis.pdf;/Users/davidnicholson/Zotero/storage/C6L86Y4D/Fukuzawa - 2022 - Computational methods for a generalised acoustics .pdf;/Users/davidnicholson/Zotero/storage/TX268GES/17296.html}
}
@article{graves_framewise_2005,
  title = {Framewise Phoneme Classification with Bidirectional {{LSTM}} and Other Neural Network Architectures},
  author = {Graves, Alex and Schmidhuber, J{\"u}rgen},
  year = {2005},
  journal = {Neural networks},
  volume = {18},
  number = {5-6},
  pages = {602--610},
  doi = {10.1016/j.neunet.2005.06.042}
}
@incollection{graves_supervised_2012,
  title = {Supervised Sequence Labelling},
  booktitle = {Supervised Sequence Labelling with Recurrent Neural Networks},
  author = {Graves, Alex},
  year = {2012},
  pages = {5--13},
  publisher = {{Springer}},
  doi = {10.1007/978-3-642-24797-2_2}
}
@article{steinfathFastAccurateAnnotation,
  title = {Fast and Accurate Annotation of Acoustic Signals with Deep Neural Networks},
  author = {Steinfath, Elsa and Palacios, Adrian and Rottsch{\"a}fer, Julian and Yuezak, Deniz and Clemens, Jan},
  pages = {30},
  abstract = {Acoustic signals serve communication within and across species throughout the animal kingdom. Studying the genetics, evolution, and neurobiology of acoustic communication requires annotating acoustic signals: segmenting and identifying individual acoustic elements like syllables or sound pulses. To be useful, annotations need to be accurate, robust to noise, fast.},
  langid = {english},
  file = {/Users/davidnicholson/Zotero/storage/T7IJV8E9/Steinfath et al. - Fast and accurate annotation of acoustic signals w.pdf}
}
@techreport{zandbergBirdSongComparison2022,
  type = {Preprint},
  title = {Bird Song Comparison Using Deep Learning Trained from Avian Perceptual Judgments},
  author = {Zandberg, Lies and Morfi, Veronica and George, Julia and Clayton, David F. and Stowell, Dan and Lachlan, Robert F.},
  year = {2022},
  month = dec,
  institution = {{Animal Behavior and Cognition}},
  doi = {10.1101/2022.12.23.521425},
  urldate = {2023-01-05},
  abstract = {Abstract                        Our understanding of bird song, a model system for animal communication and the neurobiology of learning, depends critically on making reliable, validated comparisons between the complex multidimensional syllables that are used in songs. However, most assessments of song similarity are based on human inspection of spectrograms, or computational methods developed from human intuitions. Using a novel automated operant conditioning system, we collected a large corpus of zebra finches' (             Taeniopygia guttata             ) decisions about song syllable similarity. We use this dataset to compare and externally validate similarity algorithms in widely-used publicly available software (Raven, Sound Analysis Pro, Luscinia). Although these methods all perform better than chance, they do not closely emulate the avian assessments. We then introduce a novel deep learning method that can produce perceptual similarity judgements trained on such avian decisions. We find that this new method outperforms the established methods in accuracy and more closely approaches the avian assessments. Inconsistent (hence ambiguous) decisions are a common occurrence in animal behavioural data; we show that a modification of the deep learning training that accommodates these leads to the strongest performance. We argue this approach is the best way to validate methods to compare song similarity, that our dataset can be used to validate novel methods, and that the general approach can easily be extended to other species.},
  langid = {english},
  file = {/Users/davidnicholson/Zotero/storage/ALVHUX8W/Zandberg et al. - 2022 - Bird song comparison using deep learning trained f.pdf}
}
@article{paszkeAutomaticDifferentiationPyTorch2017,
  title = {Automatic Differentiation in {{PyTorch}}},
  author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  year = {2017},
  month = oct,
  urldate = {2020-08-27},
  abstract = {A summary of automatic differentiation techniques employed in PyTorch library, including novelties like support for in-place modification in presence of objects aliasing the same data, performance...},
  file = {/Users/davidnicholson/Zotero/storage/4KSDQNJX/Paszke et al. - 2017 - Automatic differentiation in PyTorch.pdf;/Users/davidnicholson/Zotero/storage/GTYCW2RJ/forum.html}
}
@misc{falconPyTorchLightning2023,
  title = {{{PyTorch Lightning}}},
  author = {Falcon, William and {team}, The PyTorch Lightning},
  year = {2023},
  month = apr,
  doi = {10.5281/zenodo.7859091},
  urldate = {2023-06-03},
  abstract = {The lightweight PyTorch wrapper for high-performance AI research. Scale your models, not the boilerplate.},
  howpublished = {Zenodo},
  keywords = {artificial intelligence,deep learning,machine learning},
  file = {/Users/davidnicholson/Zotero/storage/BX77DJKP/7859091.html}
}
@article{walt_numpy_2011,
  title = {The {{NumPy Array}}: {{A Structure}} for {{Efficient Numerical Computation}}},
  shorttitle = {The {{NumPy Array}}},
  author = {van der Walt, S. and Colbert, S. C. and Varoquaux, G.},
  year = {2011},
  month = mar,
  journal = {Computing in Science Engineering},
  volume = {13},
  number = {2},
  pages = {22--30},
  issn = {1521-9615},
  doi = {10.1109/MCSE.2011.37},
  abstract = {In the Python world, NumPy arrays are the standard representation for numerical data and enable efficient implementation of numerical computations in a high-level language. As this effort shows, NumPy performance can be improved through three techniques: vectorizing calculations, avoiding copying data in memory, and minimizing operation counts.},
  keywords = {Arrays,Computational efficiency,data structures,Finite element methods,high level language,high level languages,mathematics computing,numerical analysis,Numerical analysis,numerical computation,numerical computations,numerical data,NumPy,numpy array,Performance evaluation,programming libraries,Python,Python programming language,Resource management,scientific programming,Vector quantization}
}
@article{harris2020array,
  title = {Array Programming with {{NumPy}}},
  author = {Harris, Charles R and Millman, K Jarrod and {van der Walt}, St{\'e}fan J and Gommers, Ralf and Virtanen, Pauli and Cournapeau, David and Wieser, Eric and Taylor, Julian and Berg, Sebastian and Smith, Nathaniel J and others},
  year = {2020},
  journal = {Nature},
  volume = {585},
  number = {7825},
  pages = {357--362},
  publisher = {{Nature Publishing Group}}
}
@article{virtanen_scipy_2019,
  title = {{{SciPy}} 1.0\textendash{{Fundamental Algorithms}} for {{Scientific Computing}} in {{Python}}},
  author = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and Haberland, Matt and Reddy, Tyler and Cournapeau, David and Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and Bright, Jonathan and {van der Walt}, St{\'e}fan J. and Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and Kern, Robert and Larson, Eric and Carey, C. J. and Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and VanderPlas, Jake and Laxalde, Denis and Perktold, Josef and Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and Harris, Charles R. and Archibald, Anne M. and Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and {van Mulbregt}, Paul and Contributors, SciPy 1 0},
  year = {2019},
  month = jul,
  journal = {arXiv:1907.10121 [physics]},
  eprint = {1907.10121},
  primaryclass = {physics},
  urldate = {2019-08-17},
  abstract = {SciPy is an open source scientific computing library for the Python programming language. SciPy 1.0 was released in late 2017, about 16 years after the original version 0.1 release. SciPy has become a de facto standard for leveraging scientific algorithms in the Python programming language, with more than 600 unique code contributors, thousands of dependent packages, over 100,000 dependent repositories, and millions of downloads per year. This includes usage of SciPy in almost half of all machine learning projects on GitHub, and usage by high profile projects including LIGO gravitational wave analysis and creation of the first-ever image of a black hole (M87). The library includes functionality spanning clustering, Fourier transforms, integration, interpolation, file I/O, linear algebra, image processing, orthogonal distance regression, minimization algorithms, signal processing, sparse matrix handling, computational geometry, and statistics. In this work, we provide an overview of the capabilities and development practices of the SciPy library and highlight some recent technical developments.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Mathematical Software,Computer Science - Software Engineering,Physics - Computational Physics}
}
@article{Hunter:2007,
  title = {Matplotlib: {{A 2D}} Graphics Environment},
  author = {Hunter, J. D.},
  year = {2007},
  journal = {Computing in Science \& Engineering},
  volume = {9},
  number = {3},
  pages = {90--95},
  publisher = {{IEEE COMPUTER SOC}},
  doi = {10.1109/MCSE.2007.55},
  abstract = {Matplotlib is a 2D graphics package used for Python for application development, interactive scripting, and publication-quality image generation across user interfaces and operating systems.}
}
@misc{thomas_a_caswell_2020_4030140,
  title = {Matplotlib/Matplotlib: {{REL}}: V3.3.2},
  author = {Caswell, Thomas A and Droettboom, Michael and Lee, Antony and Hunter, John and {de Andrade}, Elliott Sales and Firing, Eric and Hoffmann, Tim and Klymak, Jody and Stansby, David and Varoquaux, Nelle and Nielsen, Jens Hedegaard and Root, Benjamin and May, Ryan and Elson, Phil and Sepp{\"a}nen, Jouni K. and Dale, Darren and Lee, Jae-Joon and McDougall, Damon and Straw, Andrew and Hobson, Paul and Gohlke, Christoph and Yu, Tony S and Ma, Eric and Vincent, Adrien F. and Silvester, Steven and Moad, Charlie and Kniazev, Nikita and {hannah} and Ernest, Elan and Ivanov, Paul},
  year = {2020},
  month = sep,
  doi = {10.5281/zenodo.4030140},
  howpublished = {Zenodo}
}
@misc{team_pandas-devpandas_2020,
  title = {Pandas-Dev/Pandas: {{Pandas}}},
  author = {pandas development {team}, The},
  year = {2020},
  month = feb,
  publisher = {{Zenodo}},
  doi = {10.5281/zenodo.3509134}
}
@book{dask_development_team_dask_2016,
  title = {Dask: {{Library}} for Dynamic Task Scheduling},
  author = {{Dask Development Team}},
  year = {2016}
}
@article{nicholson2023crowsetta,
  title={Crowsetta: A Python tool to work with any format for annotating animal vocalizations and bioacoustics data.},
  author={Nicholson, David},
  journal={Journal of Open Source Software},
  volume={8},
  number={84},
  pages={5338},
  year={2023},
  doi={10.21105/joss.05338}
}
@misc{torchvision2016,
  title = {{{TorchVision}}: {{PyTorch}}'s Computer Vision Library},
  author = {{maintainers}, TorchVision and {contributors}},
  year = {2016},
  howpublished = {GitHub}
}
@inproceedings{lea2017temporal,
  title={Temporal convolutional networks for action segmentation and detection},
  author={Lea, Colin and Flynn, Michael D and Vidal, Rene and Reiter, Austin and Hager, Gregory D},
  booktitle={proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={156--165},
  year={2017},
  doi = {10.1109/cvpr.2017.113}
}
@article{mcinnes2018umap,
  title={Umap: Uniform manifold approximation and projection for dimension reduction},
  author={McInnes, Leland and Healy, John and Melville, James},
  journal={arXiv preprint arXiv:1802.03426},
  year={2018}
}
@article{sainburg2021parametric,
  title={Parametric UMAP embeddings for representation and semisupervised learning},
  author={Sainburg, Tim and McInnes, Leland and Gentner, Timothy Q},
  journal={Neural Computation},
  volume={33},
  number={11},
  pages={2881--2907},
  year={2021},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…},
  doi={10.1162/neco_a_01434}
}
@article{nicholson_bengalese_2017,
  title = {Bengalese {{Finch}} Song Repository},
  author = {Nicholson, David and Queen, Jonah E. and Sober, Samuel J.},
  year = {2017},
  month = oct,
  doi = {10.6084/m9.figshare.4805749.v5},
  urldate = {2019-08-17},
  abstract = {This is a collection of song from four Bengalese finches recorded in the Sober lab at Emory University. The song has been hand-labeled by two of the authors. To make it easy to work with the dataset, we have created a Python package, "evfuncs", available at https://github.com/soberlab/evfuncs (Please see "References" section below for a direct link).How to work with the files is described on the README of that library, but we describe the types of files here briefly. The actual sound files have the extension .cbin and were created by an application that runs behavioral experiments and collects data called EvTAF. Each .cbin file has an associated .cbin.not.mat file that contains song syllable onsets, offsets, labels, etc., created by a GUI for song annotation called evsonganaly. Each .cbin file also has associated .tmp and .rec files, also created by EvTAF. Those files are not strictly required to work with this dataset but are included for completeness.We share this collection as a means of testing different machine learning algorithms for classifying the elements of birdsong, known as syllables. A Python package for that purpose, "hybrid-vocal-classifier", was developed in part using this dataset.To learn more about hybrid-vocal-classifier, please visit https://hybrid-vocal-classifier.readthedocs.io/en/latest/ (see "References" section below for a direct link).},
  keywords = {Bengalese finch,Bengalese Finch Birdsongs,Bengalese finch song,Keras,machine learning,neuroscience/behavioral neuroscience,NumPy,Python,scikit-learn,SciPy,songbird studies,songbirds}
}
@misc{nicholson_vocalpyvocalpy_2023,
	title = {vocalpy/vocalpy: 0.3.0},
	shorttitle = {vocalpy/vocalpy},
	url = {https://zenodo.org/record/7925888},
	abstract = {What's Changed Add initial plot module by @NickleDave in https://github.com/vocalpy/vocalpy/pull/35 Add MVP of SequenceDataset saving to / loading from SQLite by @NickleDave in https://github.com/vocalpy/vocalpy/pull/36 Full Changelog: https://github.com/vocalpy/vocalpy/compare/0.2.0...0.3.0},
	urldate = {2023-07-16},
	publisher = {Zenodo},
	author = {Nicholson, David},
	month = may,
	year = {2023},
	doi = {10.5281/zenodo.7925888},
	file = {Zenodo Snapshot:/home/pimienta/Zotero/storage/T8GR265Q/7925888.html:text/html},
}